{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMb0k2U/PsM4NvA5ru58TCo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#!pip install \"gymnasium[box2d]\"\n","!pip install Box2D\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZYWmt5uFO5t_","executionInfo":{"status":"ok","timestamp":1763525783128,"user_tz":-480,"elapsed":6055,"user":{"displayName":"CHOO JUN TAN","userId":"08239778815350167935"}},"outputId":"30e85efc-9480-400a-d598-c143b211bb7b"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Box2D in /usr/local/lib/python3.12/dist-packages (2.3.10)\n"]}]},{"cell_type":"code","execution_count":59,"metadata":{"id":"Rtr_57jVq1Qb","executionInfo":{"status":"ok","timestamp":1763525783286,"user_tz":-480,"elapsed":156,"user":{"displayName":"CHOO JUN TAN","userId":"08239778815350167935"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"48bb895d-64a8-4a56-c0d4-60467e6032e4"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  return datetime.utcnow().replace(tzinfo=utc)\n"]}],"source":["import gymnasium as gym # Use gymnasium instead of gym for numpy compatibility\n","import numpy as np # numerical operations\n","import random # random sampling\n","from collections import deque # replay memory\n","import tensorflow as tf # tensorflow for neural network\n","from tensorflow.keras import models, layers, optimizers\n","import Box2D"]},{"cell_type":"code","source":["class DQNAgent:\n","  def __init__(self, state_size, action_size):\n","    # store dimensions and initialize replay memory\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.memory = deque(maxlen=2000) # replay memory\n","    self.gamma = 0.99 # discount rate\n","    self.epsilon = 1.0 # exploration rate\n","    self.epsilon_min = 0.01\n","    self.epsilon_decay = 0.995\n","    self.learning_rate = 0.001\n","    # build main and target networks\n","    self.model = self.build_model()\n","    self.target_model = self.build_model()\n","    self.update_target_model()\n","\n","  def build_model(self):\n","    model = models.Sequential()\n","    model.add(layers.Dense(64, input_dim=self.state_size, activation='relu'))\n","    model.add(layers.Dense(64,activation='relu'))\n","    model.add(layers.Dense(self.action_size, activation='linear'))\n","    model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=self.learning_rate))\n","    return model\n","\n","  def remember(self, state, action, reward, next_state, done):\n","    self.memory.append((state, action, reward, next_state, done))\n","\n","  def act(self, state):\n","    if np.random.rand() <= self.epsilon:\n","      return random.randrange(self.action_size)\n","    act_values = self.model.predict(state, verbose=0)\n","    return np.argmax(act_values[0])\n","\n","  def replay(self, batch_size):\n","    minibatch = random.sample(self.memory, batch_size)\n","    for state, action, reward, next_state, done in minibatch:\n","      target = self.model.predict(state, verbose=0)\n","      if done:\n","        target[0][action] = reward\n","      else:\n","        t = self.target_model.predict(next_state, verbose=0)\n","        target[0][action] = reward + self.gamma * np.amax(t[0])\n","      self.model.fit(state, target, epochs=1, verbose=0)\n","\n","    if self.epsilon > self.epsilon_min:\n","      self.epsilon *= self.epsilon_decay\n","\n","  def update_target_model(self):\n","    self.target_model.set_weights(self.model.get_weights())"],"metadata":{"id":"H2vcudMHIYlx","executionInfo":{"status":"ok","timestamp":1763525786569,"user_tz":-480,"elapsed":26,"user":{"displayName":"CHOO JUN TAN","userId":"08239778815350167935"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["env = gym.make(\"LunarLander-v3\")\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n\n","agent = DQNAgent(state_size, action_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tzSzIDmVR9xE","executionInfo":{"status":"ok","timestamp":1763525803176,"user_tz":-480,"elapsed":155,"user":{"displayName":"CHOO JUN TAN","userId":"08239778815350167935"}},"outputId":"975f80bf-0ed2-4694-d7c2-d1b16424b41d"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"hMMXIh5oN8Lo"}},{"cell_type":"code","source":["episodes = 300\n","batch_size = 64\n","\n","for e in range(episodes):\n","  state, _ = env.reset() # Unpack observation and info\n","  state = np.reshape(state,[1, state_size])\n","  total_reward = 0\n","\n","  for time in range(500):\n","    action = agent.act(state)\n","    next_state, reward, done, _, _ = env.step(action) # Correctly unpack 5 values\n","    next_state = np.reshape(next_state, [1, state_size])\n","    agent.remember(state, action, reward, next_state, done)\n","    state = next_state\n","    total_reward += reward\n","    if done:\n","      agent.update_target_model()\n","      print(f\"Episodes: {e+1}/{episodes}, Score: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n","      break\n","    if len(agent.memory) > batch_size:\n","      agent.replay (batch_size)"],"metadata":{"id":"WVm_2ZVuPyFe"},"execution_count":null,"outputs":[]}]}
