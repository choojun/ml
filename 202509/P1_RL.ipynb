{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtgjXGD6N-bs",
        "outputId": "cc4e14f7-5460-4848-882a-ff29e3b28c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GymEnvironment:\n",
        "    def __init__(self, env_name=\"CartPole-v1\"):\n",
        "        self.env = gym.make(env_name)\n",
        "        self.state, self.info = self.env.reset()\n",
        "        self.name = env_name\n",
        "        print(f\"Environment'{env_name}' initialized.\")\n",
        "\n",
        "    def do(self, action):\n",
        "        next_state,reward, terminated, truncated, info = self.env.step(action) # Corrected truncted to truncated\n",
        "        done = terminated or truncated\n",
        "        self.state = next_state\n",
        "        return reward, next_state, done, info"
      ],
      "metadata": {
        "id": "fMuPpAPiOEHu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "my_custom_env_instance = GymEnvironment(env_name=\"CartPole-v1\")\n",
        "\n",
        "action = my_custom_env_instance.env.action_space.sample()\n",
        "\n",
        "reward, next_state, done, info = my_custom_env_instance.do(action)\n",
        "print(f\"Action: {action}, Reward: {reward}, Done: {done}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMwaw3x0OIH2",
        "outputId": "e4a7b6e7-937c-404d-fb49-cae378a81a5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment'CartPole-v1' initialized.\n",
            "Action: 0, Reward: 1.0, Done: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RLAgent:\n",
        "    def __init__(self, actions):\n",
        "        self.actions = actions\n",
        "        self.state = None\n",
        "        self.reward = 0\n",
        "        self.action = None\n",
        "        self.name = \"Random Agent\"\n",
        "\n",
        "    def select_action(self, env_state):\n",
        "        self.state = env_state\n",
        "        self.action = random.choice(self.actions)\n",
        "        return self.action\n",
        "\n",
        "    def v(self, state):\n",
        "        return 0\n",
        "\n",
        "    def q(self, action):\n",
        "        return 0"
      ],
      "metadata": {
        "id": "4eir2X1eRMX7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tXn916hhROHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = GymEnvironment(env_name=\"CartPole-v1\")\n",
        "agent = RLAgent(actions = list(range(env.env.action_space.n)))\n",
        "\n",
        "state = env.state\n",
        "action = agent.select_action(state)\n",
        "reward, next_state, done, info = env.do(action)\n",
        "print(f\"Initial action: {action}, Reward: {reward}\")\n",
        "\n",
        "next_action = agent.select_action(next_state)\n",
        "print(f\"Next action: {next_action}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILY4jxKARPmr",
        "outputId": "76ee1af7-bcc8-4155-f56f-dc84ca089311"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment'CartPole-v1' initialized.\n",
            "Initial action: 0, Reward: 1.0\n",
            "Next action: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Simulation:\n",
        "    def __init__(self, agent, envionment):\n",
        "        self.agent = agent\n",
        "        self.env = envionment\n",
        "        self.reward_history = [] # Initialized reward_history as an empty list\n",
        "        self.total_reward = 0\n",
        "        self.steps = 0\n",
        "        self.action = None # Initialize action here or in start\n",
        "\n",
        "    def start(self):\n",
        "        self.action = self.agent.select_action(self.env.state) # Corrected typo (enf to env) and method call\n",
        "        return self\n",
        "\n",
        "    def go(self, n=10):\n",
        "        for i in range(n):\n",
        "            reward, next_state, done, info = self.env.do(self.action)\n",
        "            self.reward_history.append(reward) # Corrected typo (self_reward_history to self.reward_history)\n",
        "            self.total_reward += reward\n",
        "            self.steps += 1\n",
        "            if done:\n",
        "                # Reset the environment and update the agent's perception of the state\n",
        "                self.env.state, self.env.info = self.env.env.reset()\n",
        "\n",
        "            # Select the next action based on the current environment state (which might have been reset)\n",
        "            self.action = self.agent.select_action(self.env.state) # Corrected arguments for select_action\n",
        "\n",
        "        print(f\"Total reward after {n} steps: {self.total_reward}\")\n",
        "        return self"
      ],
      "metadata": {
        "id": "jfPjtTceUGv_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GymEnvironment(env_name=\"CartPole-v1\")\n",
        "agent = RLAgent(actions=list(range(env.env.action_space.n)))\n",
        "sim = Simulation(agent, env)\n",
        "sim.start().go(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fgEXxiSUK4Y",
        "outputId": "75862e02-65c9-4192-ae70-dabe608e1627"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment'CartPole-v1' initialized.\n",
            "Total reward after 10 steps: 10.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Simulation at 0x7aa4d89679e0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}
