{"cells":[{"cell_type":"markdown","metadata":{"id":"Fqj5NtqXc778"},"source":["# Part 1: Introduction to Pipeline\n","\n","Scikit-learn's Pipeline class is designed apply a series of data transformations followed by the application of an estimator.\n","\n","Benefits:\n","- Convenience in creating an easy-to-understand workflow.\n","- Enforcing workflow implementation and the desired order of step applications.\n","- Reproducibility."]},{"cell_type":"markdown","metadata":{"id":"TRAkBgR0c78A"},"source":["---\n","In this notebook, we will perform the following:\n","\n","Build **3 pipelines**, each with a different **Estimators**, using default hyperparameters:\n","\n","- Logistic Regression\n","- Support Vector Machine\n","- Decision Tree\n","\n","Build a **pipeline** for **Transform**, consisting of:\n","\n","- Feature Scaling\n","- Dimensionality Reduction (PCA)\n","\n","Then, the data is fitted to the final estimators."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"EJsuF0IGc78B","executionInfo":{"status":"ok","timestamp":1739117128097,"user_tz":-480,"elapsed":4424,"user":{"displayName":"SIEW MOOI LIM","userId":"08035534562801716990"}}},"outputs":[],"source":["# Importing the dataset\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","iris = load_iris()\n","X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"s8Mbvzh7c78C"},"source":["- We will construct pipelines for Logistic Regression, Support Vector Machine and Decision Tree\n","- `StandardScaler()` is used to resize the distribution of values such that the mean value is 0 and the standard deviation is 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZyW0_zpfc78D"},"outputs":[],"source":["# Import the required packages\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import svm\n","from sklearn import tree\n","\n","# Construct the Pipelines\n","pipe_logreg = Pipeline([('scaler', StandardScaler()),\n","                        ('pca', PCA(n_components=2)),\n","                        ('clf', LogisticRegression(random_state=42))])\n","\n","pipe_svm = Pipeline([('scaler', StandardScaler()),\n","                        ('pca', PCA(n_components=2)),\n","                        ('svm', svm.SVC(random_state=42))])\n","\n","pipe_tree = Pipeline([('scaler', StandardScaler()),\n","                        ('pca', PCA(n_components=2)),\n","                        ('clf', tree.DecisionTreeClassifier(random_state=42))])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rR1rWmtAc78D"},"outputs":[],"source":["# List of pipelines for ease of iteration\n","pipelines = [pipe_logreg, pipe_svm, pipe_tree]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"knIKHfixc78E"},"outputs":[],"source":["# Dictionary of Pipelines and Classifiers for ease of reference\n","pipe_dict = {0: 'Logistic Regression', 1: 'Support Vector Machine', 2: 'Decision Tree'}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YU4S3d6Uc78E"},"outputs":[],"source":["# Fit the pipelines\n","for pipe in pipelines:\n","    pipe.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lDOrDleZc78E","outputId":"c34ff700-ce4e-495a-8f63-9d681afb4982"},"outputs":[{"name":"stdout","output_type":"stream","text":["Logistic Regression pipeline test accuracy: 0.900\n","Support Vector Machine pipeline test accuracy: 0.900\n","Decision Tree pipeline test accuracy: 0.867\n"]}],"source":["# Compare accuracies\n","for idx, val in enumerate (pipelines):\n","    print(\"{} pipeline test accuracy: {:.3f}\".format(pipe_dict[idx], val.score(X_test, y_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v86N19vNc78F","outputId":"7a046e7e-d972-4eb0-d9e1-b07974517bb8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Classifier with best accuracy: Decision Tree\n"]}],"source":["# Identify the most accurate model on test data\n","best_accuracy = 0.0\n","best_clf = 0\n","best_pipe = ''\n","\n","for idx, val in enumerate (pipelines):\n","    if val.score(X_test, y_test) > best_accuracy:\n","        best_accuracy = val.score(X_test, y_test)\n","        best_pipe = val\n","        best_clf = idx\n","\n","print(\"Classifier with best accuracy: {}\".format(pipe_dict[idx]))"]},{"cell_type":"markdown","metadata":{"id":"414in7ICc78F"},"source":["# Part 2: Integrating Grid Search\n","\n","Another simple yet powerful technique we can pair with pipelines to improve performance is **grid search**, which attempts to optimize model hyperparameter combinations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYt79n5Uc78G"},"outputs":[],"source":["from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","# Load and split the data\n","iris = load_iris()\n","X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dvs4dnnfc78G"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn import tree\n","\n","# Construct Pipeline\n","pipe = Pipeline([('scaler', StandardScaler()),\n","                 ('pca', PCA(n_components=2)),\n","                 ('clf', tree.DecisionTreeClassifier(random_state=42))])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_P0Q6mEVc78G"},"outputs":[],"source":["# Fit the Pipeline\n","pipe.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwMMmbbCc78G"},"outputs":[],"source":["# Pipeline Test Accuracy\n","print(\"Test Accuracy: {:.3f}\".format(pipe.score(X_test, y_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bp2fXlK6c78G"},"outputs":[],"source":["# Pipeline estimator parameters\n","# Estimator is stored as step 3 ([2]), second item ([1])\n","print(\"Model hyperparameters: \\n{}\".format(pipe.steps[2][1].get_params()))"]},{"cell_type":"markdown","metadata":{"id":"w2pC086gc78H"},"source":["As summary, we applied feature **scaling (scaler)**, **dimensionality reduction (pca)**, and applied the **final estimator (clf)**"]},{"cell_type":"markdown","metadata":{"id":"f_JixI55c78H"},"source":["## Part 2.1 Adding Grid Search to the Pipeline\n","\n","The purpose of grid search is to locate the optimal hyperparameters to optimize the model's accuracy. Grid Search will be applied to optimize the following hyperparameters:\n","\n","- **criterion** - This is the function to evaluate the quality of the split; Gini **impurity** and **information gain (entropy)**\n","- **min_samples_leaf** - This is the minimum number of samples required for a valid leaf node; we will use the integer range of 1 to 5\n","- **max_depth** - The is the maximum depth of the tree; we will use the integer range of 1 to 5\n","- **min_samples_split** - This is the minimum number of samples required in order to split a non-leaf node; we will use the integer range of 1 to 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r4P9co_pc78H"},"outputs":[],"source":["from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","# Load and split the data\n","iris = load_iris()\n","X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lx-px_uKc78H"},"outputs":[],"source":["# Construct Pipeline\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn import tree\n","\n","# Construct Pipeline\n","pipe = Pipeline([('scaler', StandardScaler()),\n","                 ('pca', PCA(n_components=2)),\n","                 ('clf', tree.DecisionTreeClassifier(random_state=42))])\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","# Define parameter range\n","param_range = [1, 2, 3, 4, 5]\n","\n","# Set Grid Search\n","grid_params = [{'clf__criterion': ['gini', 'entropy'],\n","                'clf__min_samples_leaf': param_range,\n","                'clf__max_depth': param_range,\n","                'clf__min_samples_split': param_range[1:]}]\n","\n","# Construct Grid Search\n","grid_search = GridSearchCV(estimator=pipe,\n","                           param_grid=grid_params,\n","                           scoring='accuracy',\n","                           cv=10) # 10-fold cross validation\n","\n","# Fit using Grid Search\n","grid_search.fit(X_train, y_train)\n","\n","# Best Accuracy\n","print(\"Best accuracy: {:.3f}\".format(grid_search.best_score_))\n","\n","# Best Hyperparameters\n","print(\"Best hyperparameters:\\n {}\".format(grid_search.best_params_))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}