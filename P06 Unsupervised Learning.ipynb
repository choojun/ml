{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN6Y6sm47o2RHGO5GwpM8/m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Unsupervised Learning:**\n","\n","It involves learning patterns and structures in unlabeled data. The algorithm\n","explores the data and finds meaningful relationships or clusters without any predefined labels or\n","targets.\n"],"metadata":{"id":"FvK3J5QNZ1CT"}},{"cell_type":"markdown","source":["Popular unsupervised learning algorithms\n","\n","**1) K-Means Clustering:**"],"metadata":{"id":"3TpO5TSKaEO9"}},{"cell_type":"markdown","source":["K-Means Clustering is an unsupervised machine learning algorithm used for clustering and data\n","segmentation. It aims to partition a given dataset into K clusters, where each data point belongs\n","to the cluster with the nearest mean (centroid). The algorithm iteratively assigns data points to\n","the nearest centroid and updates the centroids until convergence."],"metadata":{"id":"_jP2cknma0Tc"}},{"cell_type":"markdown","source":["**The key hyperparameters of the K-Means Clustering algorithm:**"],"metadata":{"id":"-djmtpVVbEZ_"}},{"cell_type":"markdown","source":["**n_clusters:**\n","The number of clusters (K) to form. It determines the desired number of clusters in the\n","dataset. This parameter must be specified before running the algorithm.\n","\n","**init:**\n","The method for initializing the initial centroids. It can be set to \"k-means++\" (smart\n","initialization that improves convergence) or \"random\" (randomly selects initial centroids). The\n","default is \"k-means++\".\n","\n","**n_init:**\n","The number of times the algorithm will be run with different centroid seeds. The final result\n","will be the best output in terms of inertia. The default is 10.\n","\n","**max_iter:**\n","The maximum number of iterations for the algorithm to converge. If convergence is not\n","reached within this limit, the algorithm stops. The default is 300.\n","\n","**tol:**\n","The tolerance for convergence. If the difference in the centroids' positions between iterations\n","is less than this value, the algorithm is considered to have converged. The default is 1e-4.\n","\n","**algorithm:**\n","The algorithm used to compute the K-Means Clustering. It can be set to \"auto\", \"full\", or\n","\"elkan\". The \"auto\" option automatically selects the best algorithm based on the dataset. The\n","default is \"auto\"."],"metadata":{"id":"wo39NrntbUSO"}},{"cell_type":"code","source":["from sklearn.datasets import make_blobs\n","# Create a synthetic dataset with 100 samples and 2 features\n","X, _ = make_blobs(n_samples=100, n_features=2, centers=3, random_state=42)"],"metadata":{"id":"8Rb0G-aUcNvX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.cluster import KMeans"],"metadata":{"id":"67yJ_0XlcU3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a K-Means Clustering object with desired number of clusters (K)\n","kmeans = KMeans(n_clusters=3)\n","# Fit the K-Means model on the data\n","kmeans.fit(X)\n","# Get the cluster labels for each data point\n","labels = kmeans.labels_\n","# Get the cluster centroids\n","centroids = kmeans.cluster_centers_\n","print(\"Cluster Labels:\", labels)\n","print(\"Cluster Centroids:\", centroids)"],"metadata":{"id":"AZ4lhU9XcaU8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize the clusters using visualization libraries\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np"],"metadata":{"id":"_3QWhesdL6b8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a scatter plot for the data points\n","plt.figure(figsize=(7, 4))\n","\n","# Plot the data points with color corresponding to their cluster label\n","scatter = plt.scatter(X[:, 0], X[:, 1],\n","                      c=labels,\n","                      cmap='viridis',\n","                      s=50,\n","                      alpha=0.6,\n","                      edgecolor='k')\n","\n","# Plot the cluster centroids\n","plt.scatter(centroids[:, 0], centroids[:, 1],\n","            c='red',\n","            s=200,\n","            marker='X',\n","            label='Centroids')\n","# Create a custom legend for clusters\n","# Generate a list of cluster labels\n","unique_labels = np.unique(labels)\n","# Create a legend entry for each cluster\n","handles = [plt.Line2D([0], [0],\n","                      marker='o',\n","                      color='w',\n","                      markerfacecolor=plt.cm.viridis(i / (len(unique_labels) - 1)),\n","                      markersize=10,\n","                      label=f'Cluster {i}') for i in unique_labels]\n","# Add custom legend handles to the plot\n","plt.legend(handles=handles + [plt.Line2D([0], [0],\n","                                         marker='X',\n","                                         color='w',\n","                                         markerfacecolor='red',\n","                                         markersize=11,\n","                                         label='Centroids')])\n","# Add titles and labels\n","plt.title('K-Means Clustering')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.show()"],"metadata":{"id":"FcUwKiHYavn0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2) Agglomerative Clustering (Hierarchical Clustering)**\n","\n","Hierarchical Clustering is an unsupervised machine learning algorithm used for clustering and\n","data segmentation. It builds a hierarchy of clusters by iteratively merging or splitting clusters\n","based on a distance or similarity metric. There are two main types of hierarchical clustering:\n","Agglomerative and Divisive."],"metadata":{"id":"iaI3ukRccrhX"}},{"cell_type":"markdown","source":["**Here are the key hyperparameters for hierarchical clustering:**"],"metadata":{"id":"5u2tVSUTc-jQ"}},{"cell_type":"markdown","source":["**n_clusters:**\n","The desired number of clusters to form. This parameter is optional and specifies the number\n","of clusters to be formed at the end of the clustering process. If not provided, the algorithm\n","will create clusters based on a distance threshold or another stopping criterion.\n","\n","**affinity:**\n","The distance or similarity metric used to compute the proximity between data points. It can\n","be set to \"euclidean\", \"manhattan\", \"cosine\", or other valid distance measures. The default is\n","\"euclidean\".\n","\n","**linkage:**\n","The linkage criterion used to determine how to merge or split clusters. It can be set to\n","\"ward\", \"complete\", \"average\", or \"single\". The default is \"ward\", which minimizes the variance\n","of the clusters being merged.\n","\n","**distance_threshold:**\n","The threshold to use when forming flat clusters. This parameter is optional and allows you to\n","specify a distance value at which clusters are formed. Clusters that have a distance less than\n","or equal to this threshold are merged into a single cluster."],"metadata":{"id":"ChFLk1SLdDaj"}},{"cell_type":"code","source":["from sklearn.cluster import AgglomerativeClustering\n","# Create an Agglomerative Clustering object with desired number of clusters (n_clusters)\n","hc = AgglomerativeClustering(n_clusters=3, linkage='ward')\n","# Fit the hierarchical clustering model on the data\n","hc.fit(X)\n","# Get the cluster labels for each data point\n","labels = hc.labels_\n","print(\"Cluster Labels:\", labels)"],"metadata":{"id":"Z8Dk-KwLdhm5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Visualize the clusters using visualization libraries.\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","from sklearn.cluster import AgglomerativeClustering"],"metadata":{"id":"c6cpHptKMBLs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute the linkage matrix\n","# Note: `linkage` function needs to be applied on the data to create the dendrogram\n","Z = linkage(X, method='average')\n","\n","# Plot the dendrogram\n","plt.figure(figsize=(8, 5))\n","dendrogram(Z)\n","plt.title('Dendrogram')\n","plt.xlabel('Sample index')\n","plt.ylabel('Distance')\n","plt.show()"],"metadata":{"id":"3eoyWXSybKP6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8, 5))\n","\n","# Plot the data points with color corresponding to their cluster label\n","scatter = plt.scatter(X[:, 0], X[:, 1],\n","                      c=labels,\n","                      cmap='viridis',\n","                      s=50,\n","                      alpha=0.6,\n","                      edgecolor='k')\n","# Create a custom legend for clusters\n","# Generate a list of cluster labels\n","unique_labels = np.unique(labels)\n","# Create a legend entry for each cluster\n","handles = [plt.Line2D([0], [0],\n","                      marker='o',\n","                      color='w',\n","                      markerfacecolor=plt.cm.viridis(i / (len(unique_labels) - 1)),\n","                      markersize=9,\n","                      label=f'Cluster {i}') for i in unique_labels]\n","\n","# Add custom legend handles to the plot\n","plt.legend(handles=handles)\n","\n","# Add titles and labels\n","plt.title('Agglomerative Clustering')\n","plt.xlabel('Feature 1')\n","plt.ylabel('Feature 2')\n","plt.show()"],"metadata":{"id":"KjPvHv1dbT-z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**3) DBSCAN (Density-Based Spatial Clustering of Applications with\n","Noise):**\n","\n","DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is an unsupervised\n","machine learning algorithm used for clustering and data segmentation. It groups together data\n","points that are close to each other in dense regions, while labeling data points in sparser regions\n","as noise or outliers. DBSCAN does not require the number of clusters to be specified in advance\n","and can discover clusters of arbitrary shapes."],"metadata":{"id":"SAZlR84GdwD-"}},{"cell_type":"markdown","source":["**The key hyperparameters for DBSCAN:**"],"metadata":{"id":"L8Jlp2UXd1yQ"}},{"cell_type":"markdown","source":["**eps:**\n","The maximum distance between two data points to be considered neighbors. It defines the\n","radius of the neighborhood around each data point. Points within this distance are\n","considered part of the same cluster. The default value is 0.5.\n","\n","**min_samples:**\n","The minimum number of data points required to form a dense region. Points that have at\n","least min_samples neighbors within the eps radius are considered core points and are used\n","to form clusters. The default value is 5."],"metadata":{"id":"URRqrjuId5NO"}},{"cell_type":"markdown","source":["**metric:**\n","The distance metric used to compute the distance between data points. It can be set to a\n","valid string identifier or a callable function that takes two arrays and returns a distance\n","value. The default is \"euclidean\".\n","\n","**algorithm:**\n","The algorithm used to compute the DBSCAN clustering. It can be set to \"auto\", \"ball_tree\",\n","\"kd_tree\", or \"brute\". The \"auto\" option automatically selects the best algorithm based on\n","the dataset. The default is \"auto\".\n","\n","**leaf_size:**\n","The leaf size passed to the ball_tree or kd_tree algorithms. It affects the speed and memory\n","usage of the algorithm. The default is 30.\n","\n","**n_jobs:**\n","The number of parallel jobs to run for neighbor search. It can speed up the computation for\n","large datasets. Setting it to -1 uses all available processors. The default is 1."],"metadata":{"id":"Ng2DJBt4eE4o"}},{"cell_type":"code","source":["from sklearn.cluster import DBSCAN\n","# Create a DBSCAN object with desired epsilon (eps) and minimum samples (min_samples)\n","dbscan = DBSCAN(eps=0.5, min_samples=5)\n","# Fit the DBSCAN model on the data\n","dbscan.fit(X)\n","# Get the cluster labels for each data point\n","labels = dbscan.labels_\n","print(\"Cluster Labels:\", labels)"],"metadata":{"id":"gR_fAqH3edAH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize the clusters using visualization libraries.\n","\n"],"metadata":{"id":"qQ3eH-DiMFqc"},"execution_count":null,"outputs":[]}]}