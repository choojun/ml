{"cells":[{"cell_type":"markdown","metadata":{"id":"fg2rl5SKZ-q1"},"source":["## Exercise: Using LSTMs to Classify the 20 Newsgroups Data Set\n","The 20 Newsgroups data set is a well known classification problem. The goal is to classify which newsgroup a particular post came from.  The 20 possible groups are:\n","\n","`comp.graphics\n","comp.os.ms-windows.misc\n","comp.sys.ibm.pc.hardware\n","comp.sys.mac.hardware\n","comp.windows.x\trec.autos\n","rec.motorcycles\n","rec.sport.baseball\n","rec.sport.hockey\n","sci.crypt\n","sci.electronics\n","sci.med\n","sci.space\n","misc.forsale\n","talk.politics.misc\n","talk.politics.guns\n","talk.politics.mideast\n","talk.religion.misc\n","alt.atheism\n","soc.religion.christian`\n","\n","As you can see, some pairs of groups may be quite similar while others are very different.\n","\n","The data is given as a designated training set of size 11314 and test set of size 7532.  The 20 categories are represented in roughly equal proportions, so the baseline accuracy is around 5%.\n"]},{"cell_type":"markdown","metadata":{"id":"sYL9mFPDZ-q4"},"source":["To begin, review the code below.  This will walk you through the basics of loading in the 20 newsgroups data, loading in the GloVe data, building the word embedding matrix, and building the LSTM model.\n","\n","After we build the first LSTM model, it will be your turn to build one and play with the parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELRMyDNQZ-q5"},"outputs":[],"source":["import numpy as np\n","\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding\n","from keras.layers import LSTM\n","\n","import keras\n","\n","from sklearn.datasets import fetch_20newsgroups\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HLe3vImsZ-q6"},"outputs":[],"source":["max_features = 20000\n","seq_length = 30  # How long to make our word sequences\n","batch_size = 32\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PK8lMXtZ-q6"},"outputs":[],"source":["# Download the 20 newsgroups data - there is already a designated \"train\" and \"test\" set\n","\n","newsgroups_train = fetch_20newsgroups(subset='train')\n","newsgroups_test = fetch_20newsgroups(subset='test')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTkTGGokZ-q7"},"outputs":[],"source":["len(newsgroups_train.data), len(newsgroups_test.data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-j5alugZ-q7"},"outputs":[],"source":["tokenizer = Tokenizer(num_words=max_features)\n","tokenizer.fit_on_texts(newsgroups_train.data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R9phgeEcZ-q7"},"outputs":[],"source":["sequences_train = tokenizer.texts_to_sequences(newsgroups_train.data)\n","sequences_test = tokenizer.texts_to_sequences(newsgroups_test.data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3xJcZMbuZ-q8","outputId":"b0c752dc-be82-48da-cdcf-266d93db3823"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 134142 unique tokens.\n"]}],"source":["word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z55Y0WoYZ-q9"},"outputs":[],"source":["x_train = pad_sequences(sequences_train, maxlen=seq_length)\n","x_test = pad_sequences(sequences_test, maxlen=seq_length)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkOG_zuPZ-q9","outputId":"fc5d8095-257b-4527-ec0f-48c24dc03c40"},"outputs":[{"data":{"text/plain":["array([[ 2908,   198,     3, ...,    35,    58,  7860],\n","       [  351,   138,   533, ...,   118,   441,    15],\n","       [    9,    33,     4, ...,   187,    84, 17015],\n","       ...,\n","       [   10,     1,  1787, ...,   349,   383,    31],\n","       [  115,   362,    67, ...,  7772,   486,   492],\n","       [ 4485, 13919,  1031, ...,   200,    38,  3826]])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["x_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BEukIkdsZ-q9"},"outputs":[],"source":["y_train = keras.utils.to_categorical(np.asarray(newsgroups_train.target))\n","y_test = keras.utils.to_categorical(np.asarray(newsgroups_test.target))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjU_ToWuZ-q-"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"a4yf0LKXZ-q-"},"source":["We will be using the Glove pre-trained word vectors.  If you haven't already, please download them using this link:\n","(NOTE: this will start downloading an 822MB file)\n","\n","http://nlp.stanford.edu/data/glove.6B.zip\n","\n","Then unzip the file and fill your local path to the file in the code cell below.\n","\n","We will use the file `glove.6B.100d.txt`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSJctDiYZ-q-","outputId":"1784ba26-750d-4674-b25b-37df5fec0e73"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 400000 word vectors.\n"]}],"source":["embeddings_index = {}\n","f = open('glove.6B.100d.txt','r', encoding=\"utf-8\")\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('Found %s word vectors.' % len(embeddings_index))"]},{"cell_type":"markdown","metadata":{"id":"3DVJFBdxZ-q-"},"source":["Let's just look at a word embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h9ZXgDuXZ-q-"},"outputs":[],"source":["dog_vec = embeddings_index['dog']\n","dog_vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0PS5JdE-Z-q-"},"outputs":[],"source":["## This creates a matrix where the $i$th row gives the word embedding for the word represented by integer $i$.\n","## Essentially, these will be the \"weights\" for the Embedding Layer\n","## Rather than learning the weights, we will use these ones and \"freeze\" the layer\n","\n","embedding_matrix = np.zeros((len(word_index) + 1, 100))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mr7mALOXZ-q_","outputId":"c4c372e9-63c8-488e-867a-a9e4da9e2177"},"outputs":[{"data":{"text/plain":["(134143, 100)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["embedding_matrix.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NoGS_v4HZ-q_"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"C1e3xCKdZ-q_"},"source":["## LSTM Layer\n","`keras.layers.recurrent.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)`\n","\n","- Similar in structure to the `SimpleRNN` layer\n","- `units` defines the dimension of the recurrent state\n","- `recurrent_...` refers the recurrent state aspects of the LSTM\n","- `kernel_...` refers to the transformations done on the input\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3v88Yh6Z-q_","outputId":"9698e0a2-6ef6-4c2f-cf27-bb9fce500c1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 30, 100)           13414300  \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 30)                15720     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 20)                620       \n","=================================================================\n","Total params: 13,430,640\n","Trainable params: 16,340\n","Non-trainable params: 13,414,300\n","_________________________________________________________________\n"]}],"source":["word_dimension = 100  # This is the dimension of the words we are using from GloVe\n","model = Sequential()\n","model.add(Embedding(len(word_index) + 1,\n","                            word_dimension,\n","                            weights=[embedding_matrix],  # We set the weights to be the word vectors from GloVe\n","                            input_length=seq_length,\n","                            trainable=False))  # By setting trainable to False, we \"freeze\" the word embeddings.\n","model.add(LSTM(30, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(20, activation='softmax'))\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0C3Q-3uZ-q_"},"outputs":[],"source":["rmsprop = keras.optimizers.RMSprop(lr = .002)\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=rmsprop,\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxkOV52SZ-q_","outputId":"29ba530c-c4c8-4b79-f690-36d51b873263"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train on 11314 samples, validate on 7532 samples\n","Epoch 1/20\n","11314/11314 [==============================] - 10s 920us/step - loss: 2.7524 - accuracy: 0.1445 - val_loss: 2.4695 - val_accuracy: 0.2128\n","Epoch 2/20\n","11314/11314 [==============================] - 12s 1ms/step - loss: 2.3717 - accuracy: 0.2538 - val_loss: 2.2177 - val_accuracy: 0.2993\n","Epoch 3/20\n","11314/11314 [==============================] - 12s 1ms/step - loss: 2.1776 - accuracy: 0.3204 - val_loss: 2.1056 - val_accuracy: 0.3427\n","Epoch 4/20\n","11314/11314 [==============================] - 9s 796us/step - loss: 2.0327 - accuracy: 0.3770 - val_loss: 1.9933 - val_accuracy: 0.3749\n","Epoch 5/20\n","11314/11314 [==============================] - 10s 868us/step - loss: 1.9259 - accuracy: 0.4140 - val_loss: 1.9139 - val_accuracy: 0.4006\n","Epoch 6/20\n","11314/11314 [==============================] - 9s 773us/step - loss: 1.8537 - accuracy: 0.4291 - val_loss: 1.8770 - val_accuracy: 0.4226\n","Epoch 7/20\n","11314/11314 [==============================] - 9s 782us/step - loss: 1.7996 - accuracy: 0.4482 - val_loss: 1.8445 - val_accuracy: 0.4276\n","Epoch 8/20\n","11314/11314 [==============================] - 8s 724us/step - loss: 1.7405 - accuracy: 0.4647 - val_loss: 1.8090 - val_accuracy: 0.4397\n","Epoch 9/20\n","11314/11314 [==============================] - 9s 757us/step - loss: 1.6978 - accuracy: 0.4780 - val_loss: 1.8081 - val_accuracy: 0.4438\n","Epoch 10/20\n","11314/11314 [==============================] - 9s 810us/step - loss: 1.6625 - accuracy: 0.4865 - val_loss: 1.7805 - val_accuracy: 0.4537\n","Epoch 11/20\n","11314/11314 [==============================] - 9s 804us/step - loss: 1.6267 - accuracy: 0.4986 - val_loss: 1.7893 - val_accuracy: 0.4587\n","Epoch 12/20\n","11314/11314 [==============================] - 8s 713us/step - loss: 1.5912 - accuracy: 0.5118 - val_loss: 1.7681 - val_accuracy: 0.4627\n","Epoch 13/20\n","11314/11314 [==============================] - 9s 759us/step - loss: 1.5848 - accuracy: 0.5106 - val_loss: 1.7565 - val_accuracy: 0.4677\n","Epoch 14/20\n","11314/11314 [==============================] - 10s 849us/step - loss: 1.5593 - accuracy: 0.5202 - val_loss: 1.7673 - val_accuracy: 0.4693\n","Epoch 15/20\n","11314/11314 [==============================] - 8s 720us/step - loss: 1.5366 - accuracy: 0.5271 - val_loss: 1.7600 - val_accuracy: 0.4723\n","Epoch 16/20\n","11314/11314 [==============================] - 11s 948us/step - loss: 1.5177 - accuracy: 0.5324 - val_loss: 1.7518 - val_accuracy: 0.4732\n","Epoch 17/20\n","11314/11314 [==============================] - 9s 770us/step - loss: 1.5066 - accuracy: 0.5377 - val_loss: 1.7541 - val_accuracy: 0.4738\n","Epoch 18/20\n","11314/11314 [==============================] - 8s 741us/step - loss: 1.4957 - accuracy: 0.5376 - val_loss: 1.7525 - val_accuracy: 0.4826\n","Epoch 19/20\n","11314/11314 [==============================] - 8s 715us/step - loss: 1.4902 - accuracy: 0.5402 - val_loss: 1.7519 - val_accuracy: 0.4822\n","Epoch 20/20\n","11314/11314 [==============================] - 8s 726us/step - loss: 1.4634 - accuracy: 0.5445 - val_loss: 1.7365 - val_accuracy: 0.4819\n"]},{"data":{"text/plain":["<keras.callbacks.callbacks.History at 0x1cdb1cfcdc8>"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["\n","model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=20,\n","          validation_data=(x_test, y_test))"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"fJ40cqZaZ-rA"},"source":["## Exercise\n","### Your Turn\n","- Build a neural network with a SimpleRNN instead of an LSTM (with other dimensions and parameters the same). How does the performance compare?\n","- Use the LSTM above without the pretrained word vectors (randomly initialize the weights and have them be learned during the training process).  How does the performance compare?\n","- Try different sequence lengths, and dimensions for the hidden state of the LSTM.  Can you improve the model?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vu1US9ozZ-rA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8LEGBUUyZ-rA"},"outputs":[],"source":["from keras.layers import SimpleRNN\n","from keras import initializers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oz22vTOIZ-rA","outputId":"428a8ed9-4c70-4256-e750-21a4f39d3d45"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 30, 100)           13414300  \n","_________________________________________________________________\n","simple_rnn_1 (SimpleRNN)     (None, 50)                7550      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 20)                1020      \n","=================================================================\n","Total params: 13,422,870\n","Trainable params: 8,570\n","Non-trainable params: 13,414,300\n","_________________________________________________________________\n"]}],"source":["rnn_hidden_dim=50\n","model_2 = Sequential()\n","model_2.add(Embedding(len(word_index) + 1,\n","                            100,\n","                            weights=[embedding_matrix],\n","                            input_length=seq_length,\n","                            trainable=False))\n","model_2.add(SimpleRNN(rnn_hidden_dim,\n","                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n","                    recurrent_initializer=initializers.Identity(gain=1.0),\n","                    activation='relu'))\n","\n","model_2.add(Dense(20, activation='softmax'))\n","\n","model_2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"feDNWVuiZ-rA"},"outputs":[],"source":["rmsprop = keras.optimizers.RMSprop(lr = .0002)\n","\n","model_2.compile(loss='categorical_crossentropy',\n","              optimizer=rmsprop,\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9do8A0ITZ-rA","outputId":"89fa5946-255d-461c-a2fe-23f06af1c05d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train on 11314 samples, validate on 7532 samples\n","Epoch 1/100\n","11314/11314 [==============================] - 4s 355us/step - loss: 2.5996 - accuracy: 0.1918 - val_loss: 2.4570 - val_accuracy: 0.2286\n","Epoch 2/100\n","11314/11314 [==============================] - 4s 381us/step - loss: 2.2602 - accuracy: 0.2949 - val_loss: 2.2247 - val_accuracy: 0.3093\n","Epoch 3/100\n","11314/11314 [==============================] - 4s 356us/step - loss: 2.1263 - accuracy: 0.3429 - val_loss: 2.1871 - val_accuracy: 0.3209\n","Epoch 4/100\n","11314/11314 [==============================] - 4s 338us/step - loss: 2.0366 - accuracy: 0.3743 - val_loss: 2.1642 - val_accuracy: 0.3283\n","Epoch 5/100\n","11314/11314 [==============================] - 4s 330us/step - loss: 1.9879 - accuracy: 0.3848 - val_loss: 2.0815 - val_accuracy: 0.3530\n","Epoch 6/100\n","11314/11314 [==============================] - 5s 411us/step - loss: 1.9351 - accuracy: 0.4081 - val_loss: 2.1710 - val_accuracy: 0.3405\n","Epoch 7/100\n","11314/11314 [==============================] - 4s 360us/step - loss: 1.8924 - accuracy: 0.4124 - val_loss: 2.0931 - val_accuracy: 0.3617\n","Epoch 8/100\n","11314/11314 [==============================] - 5s 419us/step - loss: 1.8602 - accuracy: 0.4271 - val_loss: 2.1082 - val_accuracy: 0.3549\n","Epoch 9/100\n","11314/11314 [==============================] - 5s 423us/step - loss: 1.8277 - accuracy: 0.4357 - val_loss: 2.0689 - val_accuracy: 0.3814\n","Epoch 10/100\n","11314/11314 [==============================] - 5s 452us/step - loss: 1.7914 - accuracy: 0.4479 - val_loss: 2.1190 - val_accuracy: 0.3674\n","Epoch 11/100\n","11314/11314 [==============================] - 5s 452us/step - loss: 1.7670 - accuracy: 0.4524 - val_loss: 2.0545 - val_accuracy: 0.3794\n","Epoch 12/100\n","11314/11314 [==============================] - 5s 425us/step - loss: 1.7419 - accuracy: 0.4631 - val_loss: 2.1138 - val_accuracy: 0.3739\n","Epoch 13/100\n","11314/11314 [==============================] - 5s 440us/step - loss: 1.7238 - accuracy: 0.4654 - val_loss: 2.1758 - val_accuracy: 0.3648\n","Epoch 14/100\n","11314/11314 [==============================] - 4s 356us/step - loss: 1.6993 - accuracy: 0.4763 - val_loss: 2.0232 - val_accuracy: 0.3937\n","Epoch 15/100\n","11314/11314 [==============================] - 4s 320us/step - loss: 1.6801 - accuracy: 0.4838 - val_loss: 2.0354 - val_accuracy: 0.4019\n","Epoch 16/100\n","11314/11314 [==============================] - 4s 368us/step - loss: 1.6603 - accuracy: 0.4841 - val_loss: 2.1007 - val_accuracy: 0.3871\n","Epoch 17/100\n","11314/11314 [==============================] - 5s 399us/step - loss: 1.6435 - accuracy: 0.4920 - val_loss: 2.2652 - val_accuracy: 0.3704\n","Epoch 18/100\n","11314/11314 [==============================] - 4s 359us/step - loss: 1.6288 - accuracy: 0.4983 - val_loss: 2.0447 - val_accuracy: 0.4032\n","Epoch 19/100\n","11314/11314 [==============================] - 4s 343us/step - loss: 1.6104 - accuracy: 0.4996 - val_loss: 2.0591 - val_accuracy: 0.3946\n","Epoch 20/100\n","11314/11314 [==============================] - 5s 414us/step - loss: 1.5906 - accuracy: 0.5108 - val_loss: 2.1259 - val_accuracy: 0.3959\n","Epoch 21/100\n","11314/11314 [==============================] - 6s 538us/step - loss: 1.5777 - accuracy: 0.5122 - val_loss: 2.1314 - val_accuracy: 0.3867\n","Epoch 22/100\n","11314/11314 [==============================] - 5s 469us/step - loss: 1.5593 - accuracy: 0.5202 - val_loss: 2.0746 - val_accuracy: 0.4035\n","Epoch 23/100\n","11314/11314 [==============================] - 5s 460us/step - loss: 1.5513 - accuracy: 0.5212 - val_loss: 2.2038 - val_accuracy: 0.3800\n","Epoch 24/100\n","11314/11314 [==============================] - 5s 421us/step - loss: 1.5350 - accuracy: 0.5230 - val_loss: 2.0867 - val_accuracy: 0.3958\n","Epoch 25/100\n","11314/11314 [==============================] - 5s 415us/step - loss: 1.5218 - accuracy: 0.5262 - val_loss: 2.2133 - val_accuracy: 0.3818\n","Epoch 26/100\n","11314/11314 [==============================] - 5s 410us/step - loss: 1.5133 - accuracy: 0.5275 - val_loss: 2.0914 - val_accuracy: 0.3976\n","Epoch 27/100\n","11314/11314 [==============================] - 6s 519us/step - loss: 1.5039 - accuracy: 0.5352 - val_loss: 2.1673 - val_accuracy: 0.3923\n","Epoch 28/100\n","11314/11314 [==============================] - 4s 382us/step - loss: 1.4934 - accuracy: 0.5315 - val_loss: 2.1304 - val_accuracy: 0.4085\n","Epoch 29/100\n","11314/11314 [==============================] - 4s 335us/step - loss: 1.4815 - accuracy: 0.5381 - val_loss: 2.1818 - val_accuracy: 0.3994\n","Epoch 30/100\n","11314/11314 [==============================] - 5s 398us/step - loss: 1.4716 - accuracy: 0.5400 - val_loss: 2.2160 - val_accuracy: 0.4077\n","Epoch 31/100\n","11314/11314 [==============================] - 4s 352us/step - loss: 1.4604 - accuracy: 0.5461 - val_loss: 2.2941 - val_accuracy: 0.3887\n","Epoch 32/100\n","11314/11314 [==============================] - 4s 347us/step - loss: 1.4583 - accuracy: 0.5442 - val_loss: 2.1461 - val_accuracy: 0.4014\n","Epoch 33/100\n","11314/11314 [==============================] - 4s 364us/step - loss: 1.4487 - accuracy: 0.5458 - val_loss: 2.2105 - val_accuracy: 0.3954\n","Epoch 34/100\n","11314/11314 [==============================] - 5s 409us/step - loss: 1.4323 - accuracy: 0.5524 - val_loss: 2.3200 - val_accuracy: 0.3951\n","Epoch 35/100\n","11314/11314 [==============================] - 4s 358us/step - loss: 1.4272 - accuracy: 0.5544 - val_loss: 2.1717 - val_accuracy: 0.4063\n","Epoch 36/100\n","11314/11314 [==============================] - 4s 328us/step - loss: 1.4137 - accuracy: 0.5596 - val_loss: 2.2253 - val_accuracy: 0.3943\n","Epoch 37/100\n","11314/11314 [==============================] - 4s 344us/step - loss: 1.4117 - accuracy: 0.5560 - val_loss: 2.2068 - val_accuracy: 0.4010\n","Epoch 38/100\n","11314/11314 [==============================] - 4s 397us/step - loss: 1.4007 - accuracy: 0.5654 - val_loss: 2.3145 - val_accuracy: 0.3963\n","Epoch 39/100\n","11314/11314 [==============================] - 4s 356us/step - loss: 1.3983 - accuracy: 0.5615 - val_loss: 2.2872 - val_accuracy: 0.4020\n","Epoch 40/100\n","11314/11314 [==============================] - 4s 334us/step - loss: 1.3905 - accuracy: 0.5615 - val_loss: 2.2912 - val_accuracy: 0.3911\n","Epoch 41/100\n","11314/11314 [==============================] - 4s 337us/step - loss: 1.3791 - accuracy: 0.5663 - val_loss: 2.4262 - val_accuracy: 0.3958\n","Epoch 42/100\n","11314/11314 [==============================] - 4s 385us/step - loss: 1.3744 - accuracy: 0.5701 - val_loss: 2.3633 - val_accuracy: 0.4040\n","Epoch 43/100\n","11314/11314 [==============================] - 4s 344us/step - loss: 1.3664 - accuracy: 0.5713 - val_loss: 2.3841 - val_accuracy: 0.4044\n","Epoch 44/100\n","11314/11314 [==============================] - 4s 341us/step - loss: 1.3590 - accuracy: 0.5732 - val_loss: 2.3226 - val_accuracy: 0.3927\n","Epoch 45/100\n","11314/11314 [==============================] - 4s 362us/step - loss: 1.3532 - accuracy: 0.5802 - val_loss: 2.2554 - val_accuracy: 0.3971\n","Epoch 46/100\n","11314/11314 [==============================] - 5s 439us/step - loss: 1.3479 - accuracy: 0.5793 - val_loss: 2.4280 - val_accuracy: 0.3867\n","Epoch 47/100\n","11314/11314 [==============================] - 5s 421us/step - loss: 1.3386 - accuracy: 0.5797 - val_loss: 2.4542 - val_accuracy: 0.3871\n","Epoch 48/100\n","11314/11314 [==============================] - 4s 392us/step - loss: 1.3375 - accuracy: 0.5792 - val_loss: 2.2761 - val_accuracy: 0.4102\n","Epoch 49/100\n","11314/11314 [==============================] - 5s 458us/step - loss: 1.3302 - accuracy: 0.5795 - val_loss: 2.3135 - val_accuracy: 0.3934\n","Epoch 50/100\n","11314/11314 [==============================] - 5s 467us/step - loss: 1.3176 - accuracy: 0.5823 - val_loss: 2.3614 - val_accuracy: 0.3870\n","Epoch 51/100\n","11314/11314 [==============================] - 5s 412us/step - loss: 1.3138 - accuracy: 0.5860 - val_loss: 2.6189 - val_accuracy: 0.3905\n","Epoch 52/100\n","11314/11314 [==============================] - 5s 458us/step - loss: 1.3093 - accuracy: 0.5887 - val_loss: 2.4494 - val_accuracy: 0.3937\n","Epoch 53/100\n","11314/11314 [==============================] - 5s 422us/step - loss: 1.3037 - accuracy: 0.5875 - val_loss: 2.3410 - val_accuracy: 0.4016\n","Epoch 54/100\n","11314/11314 [==============================] - 4s 378us/step - loss: 1.2999 - accuracy: 0.5887 - val_loss: 2.4561 - val_accuracy: 0.3907\n","Epoch 55/100\n","11314/11314 [==============================] - 5s 412us/step - loss: 1.2972 - accuracy: 0.5906 - val_loss: 2.6301 - val_accuracy: 0.3829\n","Epoch 56/100\n","11314/11314 [==============================] - 5s 421us/step - loss: 1.2923 - accuracy: 0.5935 - val_loss: 2.3594 - val_accuracy: 0.3976\n","Epoch 57/100\n","11314/11314 [==============================] - 5s 457us/step - loss: 1.2873 - accuracy: 0.5941 - val_loss: 2.4085 - val_accuracy: 0.3971\n","Epoch 58/100\n","11314/11314 [==============================] - 5s 439us/step - loss: 1.2758 - accuracy: 0.5953 - val_loss: 2.4099 - val_accuracy: 0.3990\n","Epoch 59/100\n","11314/11314 [==============================] - 6s 489us/step - loss: 1.2719 - accuracy: 0.5991 - val_loss: 2.6319 - val_accuracy: 0.3910\n","Epoch 60/100\n","11314/11314 [==============================] - 5s 423us/step - loss: 1.2727 - accuracy: 0.5975 - val_loss: 2.6552 - val_accuracy: 0.3777\n","Epoch 61/100\n","11314/11314 [==============================] - 4s 374us/step - loss: 1.2610 - accuracy: 0.6009 - val_loss: 2.4733 - val_accuracy: 0.3915\n","Epoch 62/100\n","11314/11314 [==============================] - 5s 412us/step - loss: 1.2555 - accuracy: 0.6008 - val_loss: 2.5164 - val_accuracy: 0.3927\n","Epoch 63/100\n","11314/11314 [==============================] - 5s 419us/step - loss: 1.2520 - accuracy: 0.5971 - val_loss: 2.5125 - val_accuracy: 0.3860\n","Epoch 64/100\n","11314/11314 [==============================] - 5s 422us/step - loss: 1.2473 - accuracy: 0.6011 - val_loss: 2.5844 - val_accuracy: 0.3867\n","Epoch 65/100\n","11314/11314 [==============================] - 5s 405us/step - loss: 1.2468 - accuracy: 0.6039 - val_loss: 2.6332 - val_accuracy: 0.3919\n","Epoch 66/100\n","11314/11314 [==============================] - 5s 453us/step - loss: 1.2367 - accuracy: 0.6084 - val_loss: 2.6701 - val_accuracy: 0.3885\n","Epoch 67/100\n","11314/11314 [==============================] - 5s 452us/step - loss: 1.2330 - accuracy: 0.6092 - val_loss: 2.6229 - val_accuracy: 0.3943\n","Epoch 68/100\n","11314/11314 [==============================] - 7s 608us/step - loss: 1.2306 - accuracy: 0.6094 - val_loss: 2.6133 - val_accuracy: 0.3898\n","Epoch 69/100\n","11314/11314 [==============================] - 6s 510us/step - loss: 1.2233 - accuracy: 0.6112 - val_loss: 2.6295 - val_accuracy: 0.3890\n","Epoch 70/100\n","11314/11314 [==============================] - 6s 524us/step - loss: 1.2271 - accuracy: 0.6101 - val_loss: 2.6552 - val_accuracy: 0.3923\n","Epoch 71/100\n","11314/11314 [==============================] - 7s 587us/step - loss: 1.2143 - accuracy: 0.6130 - val_loss: 2.7116 - val_accuracy: 0.3958\n","Epoch 72/100\n","11314/11314 [==============================] - 6s 515us/step - loss: 1.2067 - accuracy: 0.6176 - val_loss: 2.8686 - val_accuracy: 0.3818\n","Epoch 73/100\n","11314/11314 [==============================] - 6s 493us/step - loss: 1.2111 - accuracy: 0.6162 - val_loss: 2.8206 - val_accuracy: 0.3862\n","Epoch 74/100\n","11314/11314 [==============================] - 5s 436us/step - loss: 1.2025 - accuracy: 0.6183 - val_loss: 2.7462 - val_accuracy: 0.3833\n","Epoch 75/100\n","11314/11314 [==============================] - 4s 388us/step - loss: 1.1996 - accuracy: 0.6158 - val_loss: 2.7232 - val_accuracy: 0.3875\n","Epoch 76/100\n","11314/11314 [==============================] - 4s 388us/step - loss: 1.1978 - accuracy: 0.6205 - val_loss: 2.7793 - val_accuracy: 0.3898\n","Epoch 77/100\n","11314/11314 [==============================] - 5s 402us/step - loss: 1.1887 - accuracy: 0.6230 - val_loss: 2.7305 - val_accuracy: 0.3785\n","Epoch 78/100\n","11314/11314 [==============================] - 4s 386us/step - loss: 1.1877 - accuracy: 0.6236 - val_loss: 2.6341 - val_accuracy: 0.3934\n","Epoch 79/100\n","11314/11314 [==============================] - 4s 393us/step - loss: 1.1809 - accuracy: 0.6240 - val_loss: 2.9238 - val_accuracy: 0.3938\n","Epoch 80/100\n","11314/11314 [==============================] - 5s 439us/step - loss: 1.1817 - accuracy: 0.6228 - val_loss: 2.7394 - val_accuracy: 0.3984\n","Epoch 81/100\n","11314/11314 [==============================] - 6s 516us/step - loss: 1.1830 - accuracy: 0.6273 - val_loss: 2.8015 - val_accuracy: 0.3935\n","Epoch 82/100\n","11314/11314 [==============================] - 4s 377us/step - loss: 1.1735 - accuracy: 0.6289 - val_loss: 2.8171 - val_accuracy: 0.3814\n","Epoch 83/100\n","11314/11314 [==============================] - 5s 438us/step - loss: 1.1724 - accuracy: 0.6234 - val_loss: 2.7254 - val_accuracy: 0.3869\n","Epoch 84/100\n","11314/11314 [==============================] - 5s 400us/step - loss: 1.1694 - accuracy: 0.6275 - val_loss: 2.8971 - val_accuracy: 0.3917\n","Epoch 85/100\n","11314/11314 [==============================] - 4s 378us/step - loss: 1.1632 - accuracy: 0.6282 - val_loss: 2.8358 - val_accuracy: 0.3883\n","Epoch 86/100\n","11314/11314 [==============================] - 5s 454us/step - loss: 1.1610 - accuracy: 0.6268 - val_loss: 2.8195 - val_accuracy: 0.3821\n","Epoch 87/100\n","11314/11314 [==============================] - 4s 366us/step - loss: 1.1573 - accuracy: 0.6308 - val_loss: 2.9032 - val_accuracy: 0.3887\n","Epoch 88/100\n","11314/11314 [==============================] - 5s 407us/step - loss: 1.1557 - accuracy: 0.6297 - val_loss: 2.9649 - val_accuracy: 0.3833\n","Epoch 89/100\n","11314/11314 [==============================] - 4s 365us/step - loss: 1.1531 - accuracy: 0.6325 - val_loss: 2.9144 - val_accuracy: 0.3943\n","Epoch 90/100\n","11314/11314 [==============================] - 5s 483us/step - loss: 1.1526 - accuracy: 0.6328 - val_loss: 2.8059 - val_accuracy: 0.3903\n","Epoch 91/100\n","11314/11314 [==============================] - 5s 412us/step - loss: 1.1480 - accuracy: 0.6352 - val_loss: 2.9776 - val_accuracy: 0.3905\n","Epoch 92/100\n","11314/11314 [==============================] - 4s 358us/step - loss: 1.1460 - accuracy: 0.6322 - val_loss: 2.9655 - val_accuracy: 0.3921\n","Epoch 93/100\n","11314/11314 [==============================] - 4s 344us/step - loss: 1.1402 - accuracy: 0.6377 - val_loss: 2.8483 - val_accuracy: 0.3684\n","Epoch 94/100\n","11314/11314 [==============================] - 4s 342us/step - loss: 1.1413 - accuracy: 0.6384 - val_loss: 3.0900 - val_accuracy: 0.3837\n","Epoch 95/100\n","11314/11314 [==============================] - 5s 400us/step - loss: 1.1398 - accuracy: 0.6388 - val_loss: 2.9126 - val_accuracy: 0.3856\n","Epoch 96/100\n","11314/11314 [==============================] - 6s 546us/step - loss: 1.1340 - accuracy: 0.6393 - val_loss: 2.9265 - val_accuracy: 0.3864\n","Epoch 97/100\n","11314/11314 [==============================] - 5s 409us/step - loss: 1.1305 - accuracy: 0.6381 - val_loss: 3.0877 - val_accuracy: 0.3801\n","Epoch 98/100\n","11314/11314 [==============================] - 5s 453us/step - loss: 1.1246 - accuracy: 0.6412 - val_loss: 2.9813 - val_accuracy: 0.3865\n","Epoch 99/100\n","11314/11314 [==============================] - 5s 424us/step - loss: 1.1288 - accuracy: 0.6389 - val_loss: 3.0462 - val_accuracy: 0.3858\n","Epoch 100/100\n","11314/11314 [==============================] - 4s 376us/step - loss: 1.1205 - accuracy: 0.6442 - val_loss: 2.9763 - val_accuracy: 0.3834\n"]},{"data":{"text/plain":["<keras.callbacks.callbacks.History at 0x1cdb6d3bc88>"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["model_2.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=100,\n","          validation_data=(x_test, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qRdloYvhZ-rA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoPsCLilZ-rB","outputId":"258ef6c0-164a-46e9-9479-b35def707920"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 30, 100)           13414300  \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 30)                15720     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 20)                620       \n","=================================================================\n","Total params: 13,430,640\n","Trainable params: 16,340\n","Non-trainable params: 13,414,300\n","_________________________________________________________________\n"]}],"source":["model_3 = Sequential()\n","model_3.add(Embedding(len(word_index) + 1,\n","                            word_dimension))  # By setting trainable to False, we \"freeze\" the word embeddings.\n","model_3.add(LSTM(30, dropout=0.2, recurrent_dropout=0.2))\n","model_3.add(Dense(20, activation='softmax'))\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JfmB8q0kZ-rB"},"outputs":[],"source":["rmsprop = keras.optimizers.RMSprop(lr = .002)\n","\n","model_3.compile(loss='categorical_crossentropy',\n","              optimizer=rmsprop,\n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3FVzLjYZ-rB"},"outputs":[],"source":["model_3.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=20,\n","          validation_data=(x_test, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bl0uqQbBZ-rF"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}