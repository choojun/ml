{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Model (Scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT**\n",
    "On various instances, while working on developing a Machine Learning Model, we'll need to save our prediction models to file, and then restore them in order to reuse our previous work to.\n",
    "\n",
    "\n",
    "**WHY**\n",
    "We need to save and restore/reload later our ML Model , so as to -\n",
    "\n",
    "a) test our model on/with new data, \n",
    "\n",
    "b) compare multiple models, \n",
    "\n",
    "c) or anything else. \n",
    "\n",
    "**object serialization**\n",
    "This process / procedure of saving a ML Model is also known as object serialization - representing an object with a stream of bytes, in order to store it on disk, send it over a network or save to a database.\n",
    "\n",
    "**deserialization**\n",
    "While the restoring/reloading of ML Model procedure is known as deserialization. \n",
    "\n",
    "In this Kernel, we will explore 3 ways to Save and Reload ML Models in Python and scikit-learn, we will also discuss about the pros and cons of each method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be covering following 3 approaches of Saving and Reloading a ML Model -\n",
    "\n",
    "1) Pickle Approach\n",
    "\n",
    "2) Joblib Approach\n",
    "\n",
    "3) Manual Save and Restore to JSON approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , lets develop a ML Model which we shall use to Save and Reload in this Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ML Model Creation**\n",
    "\n",
    "For the purpose of Demo , we will create a basic Logistic Regression Model on IRIS Dataset.\n",
    "\n",
    "Dataset used : IRIS \n",
    "\n",
    "Model        : Logistic Regression using Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step - 1 ** : Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required packages \n",
    "#-------------------------\n",
    "\n",
    "# Import the Logistic Regression Module from Scikit Learn\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "\n",
    "# Import the IRIS Dataset to be used in this Kernel\n",
    "from sklearn.datasets import load_iris  \n",
    "\n",
    "# Load the Module to split the Dataset into Train & Test \n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step - 2 **: Load the IRIS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "Iris_data = load_iris()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step - 3 **: Split the IRIS Data into Training & Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(Iris_data.data, \n",
    "                                                Iris_data.target, \n",
    "                                                test_size=0.3, \n",
    "                                                random_state=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , lets build the Logistic Regression Model on the IRIS Data\n",
    "\n",
    "Note : The Model creation in this Kernel is for demonstartion only and does not cover the details of Model Creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun Kit\\.conda\\envs\\ML\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Jun Kit\\.conda\\envs\\ML\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1544: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 3.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=20,\n",
       "                   multi_class='warn', n_jobs=3, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Model\n",
    "LR_Model = LogisticRegression(C=0.1,  \n",
    "                               max_iter=20, \n",
    "                               fit_intercept=True, \n",
    "                               n_jobs=3, \n",
    "                               solver='liblinear')\n",
    "\n",
    "# Train the Model\n",
    "LR_Model.fit(Xtrain, Ytrain)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , that Model has been Created and Trained , we might want to save the trained Model for future use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 1 : Pickle approach**\n",
    "\n",
    "Following lines of code, the LR_Model which we created in the previous step is saved to file, and then loaded as a new object called Pickled_RL_Model. \n",
    "\n",
    "The loaded model is then used to calculate the accuracy score and predict outcomes on new unseen (test) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pickle Package\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Modle to file in the current working directory\n",
    "\n",
    "Pkl_Filename = \"Pickle_RL_Model.pkl\"  \n",
    "\n",
    "with open(Pkl_Filename, 'wb') as file:  \n",
    "    pickle.dump(LR_Model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=20,\n",
       "                   multi_class='warn', n_jobs=3, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Model back from file\n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    Pickled_LR_Model = pickle.load(file)\n",
    "\n",
    "Pickled_LR_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 91.11 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 0, 0, 0, 2, 2, 0, 1, 0, 0, 2, 0, 2,\n",
       "       1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 1, 2, 2, 1, 1, 0, 2, 0, 1, 0,\n",
       "       2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Reloaded Model to \n",
    "# Calculate the accuracy score and predict target values\n",
    "\n",
    "# Calculate the Score \n",
    "score = Pickled_LR_Model.score(Xtest, Ytest)  \n",
    "# Print the Score\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))  \n",
    "\n",
    "# Predict the Labels using the reloaded Model\n",
    "Ypredict = Pickled_LR_Model.predict(Xtest)  \n",
    "\n",
    "Ypredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's Reflect back on Pickle approach :**\n",
    "\n",
    "PROs of Pickle :\n",
    "\n",
    "1) save and restore our learning models is quick - we can do it in two lines of code. \n",
    "\n",
    "2) It is useful if you have optimized the model's parameters on the training data, so you don't need to repeat this step again. \n",
    "\n",
    "\n",
    "CONs of Pickle :\n",
    "\n",
    "1) it doesn't save the test results or any data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 2 - Joblib** :\n",
    "\n",
    "The Joblib Module is available from Scikit Learn package and is intended to be a replacement for Pickle, for objects containing large data. \n",
    "\n",
    "This approach will save our ML Model in the pickle format only but we dont need to load additional libraries as the 'Pickling' facility is available within Scikit Learn package itself which we will use invariably for developing our ML models.\n",
    "\n",
    "In following Python scripts , we will show how to Save and reload ML Models using Joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required Library for using Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun Kit\\.conda\\envs\\ML\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import Joblib Module from Scikit Learn\n",
    "\n",
    "from sklearn.externals import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Model using Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joblib_RL_Model.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save RL_Model to file in the current working directory\n",
    "\n",
    "joblib_file = \"joblib_RL_Model.pkl\"  \n",
    "joblib.dump(LR_Model, joblib_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the saved Model using Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=20,\n",
       "                   multi_class='warn', n_jobs=3, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load from file\n",
    "\n",
    "joblib_LR_model = joblib.load(joblib_file)\n",
    "\n",
    "\n",
    "joblib_LR_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the Saved Model using Joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 91.11 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2, 2, 2, 2, 2, 0, 0, 2, 0, 0, 0, 2, 2, 0, 1, 0, 0, 2, 0, 2,\n",
       "       1, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 1, 2, 2, 1, 1, 0, 2, 0, 1, 0,\n",
       "       2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Reloaded Joblib Model to \n",
    "# Calculate the accuracy score and predict target values\n",
    "\n",
    "# Calculate the Score \n",
    "score = joblib_LR_model.score(Xtest, Ytest)  \n",
    "# Print the Score\n",
    "print(\"Test score: {0:.2f} %\".format(100 * score))  \n",
    "\n",
    "# Predict the Labels using the reloaded Model\n",
    "Ypredict = joblib_LR_model.predict(Xtest)  \n",
    "\n",
    "Ypredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's Reflect back on Joblib approach :**\n",
    "\n",
    "PROs of Joblib :\n",
    "\n",
    "1) the Joblib library offers a bit simpler workflow compared to Pickle. \n",
    "\n",
    "2) While Pickle requires a file object to be passed as an argument, Joblib works with both file objects and string filenames. \n",
    "\n",
    "3) In case our model contains large arrays of data, each array will be stored in a separate file, but the save and restore procedure will remain the same. \n",
    "\n",
    "4) Joblib also allows different compression methods, such as 'zlib', 'gzip', 'bz2', and different levels of compression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach 3 - Manual Save and Restore to JSON ** :\n",
    "\n",
    "whenever we want to have full control over the save and restore process, the best way is to build our own functions manually.\n",
    "\n",
    "The Script following shows an example of manually saving and restoring objects using JSON. This approach allows us to select the data which needs to be saved, such as the model parameters, coefficients, training data, and anything else we need.\n",
    "\n",
    "For simplicity, we'll save only three model parameters and the training data. Some additional data we could store with this approach is, for example, a cross-validation score on the training set, test data, accuracy score on the test data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "\n",
    "import json  \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to save all of this data in a single object, one possible way to do it is to create a new class which inherits from the model class, which in our example is LogisticRegression. The new class, called MyLogReg, then implements the methods save_json and load_json for saving and restoring to/from a JSON file, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogReg(LogisticRegression):\n",
    "\n",
    "    # Override the class constructor\n",
    "    def __init__(self, C=1.0, solver='liblinear', max_iter=100, X_train=None, Y_train=None):\n",
    "        LogisticRegression.__init__(self, C=C, solver=solver, max_iter=max_iter)\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "\n",
    "    # A method for saving object data to JSON file\n",
    "    def save_json(self, filepath):\n",
    "        dict_ = {}\n",
    "        dict_['C'] = self.C\n",
    "        dict_['max_iter'] = self.max_iter\n",
    "        dict_['solver'] = self.solver\n",
    "        dict_['X_train'] = self.X_train.tolist() if self.X_train is not None else 'None'\n",
    "        dict_['Y_train'] = self.Y_train.tolist() if self.Y_train is not None else 'None'\n",
    "\n",
    "        # Creat json and save to file\n",
    "        json_txt = json.dumps(dict_, indent=4)\n",
    "        with open(filepath, 'w') as file:\n",
    "            file.write(json_txt)\n",
    "\n",
    "    # A method for loading data from JSON file\n",
    "    def load_json(self, filepath):\n",
    "        with open(filepath, 'r') as file:\n",
    "            dict_ = json.load(file)\n",
    "\n",
    "        self.C = dict_['C']\n",
    "        self.max_iter = dict_['max_iter']\n",
    "        self.solver = dict_['solver']\n",
    "        self.X_train = np.asarray(dict_['X_train']) if dict_['X_train'] != 'None' else None\n",
    "        self.Y_train = np.asarray(dict_['Y_train']) if dict_['Y_train'] != 'None' else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create an object mylogreg, pass the training data to it, and save it to file. \n",
    "\n",
    "Then we create a new object json_mylogreg and call the load_json method to load the data from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLogReg(C=1.0,\n",
       "         X_train=array([[4.3, 3. , 1.1, 0.1],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [6.4...\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.7, 2.8, 6.7, 2. ]]),\n",
       "         Y_train=array([0, 0, 1, 1, 2, 0, 1, 2, 2, 1, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2,\n",
       "       1, 0, 2, 2, 0, 1, 2, 0, 2, 1, 2, 1, 0, 2, 1, 2, 0, 2, 1, 2, 1, 2,\n",
       "       1, 1, 2, 1, 1, 2, 1, 1, 0, 2, 0, 1, 0, 1, 1, 1, 1, 0, 2, 2, 1, 1,\n",
       "       1, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2, 2, 1, 0, 0, 0, 2, 1, 0, 0, 2,\n",
       "       1, 2, 0, 0, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2]),\n",
       "         max_iter=100, solver='liblinear')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"mylogreg.json\"\n",
    "\n",
    "# Create a model and train it\n",
    "mylogreg = MyLogReg(X_train=Xtrain, Y_train=Ytrain)  \n",
    "mylogreg.save_json(filepath)\n",
    "\n",
    "# Create a new object and load its data from JSON file\n",
    "json_mylogreg = MyLogReg()  \n",
    "json_mylogreg.load_json(filepath)  \n",
    "json_mylogreg  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's reflect back on the JSON approach**\n",
    "\n",
    "PROs :\n",
    "\n",
    "Since the data serialization using JSON actually saves the object into a string format, rather than byte stream, the 'mylogreg.json' file could be opened and modified with a text editor.\n",
    "\n",
    "\n",
    "CONs :\n",
    "\n",
    "Although this approach would be convenient for the developer, it is less secure since an intruder can view and amend the content of the JSON file. \n",
    "\n",
    "Moreover, this approach is more suitable for objects with small number of instance variables, such as the scikit-learn models, because any addition of new variables requires changes in the save and restore methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new class which inherits from the model class Random Forest Classifier. The new class, called MyRF, then implements the methods save_json and load_json for saving and restoring to/from a JSON file, respectively.\n",
    "\n",
    "The random forest shall only build 10 trees with 10 as the maximum depth of each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyRF(X_train=array([[4.3, 3. , 1.1, 0.1],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [6.4, 2.9, 4.3, 1...\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.7, 2.8, 6.7, 2. ]]),\n",
       "     Y_train=array([0, 0, 1, 1, 2, 0, 1, 2, 2, 1, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2,\n",
       "       1, 0, 2, 2, 0, 1, 2, 0, 2, 1, 2, 1, 0, 2, 1, 2, 0, 2, 1, 2, 1, 2,\n",
       "       1, 1, 2, 1, 1, 2, 1, 1, 0, 2, 0, 1, 0, 1, 1, 1, 1, 0, 2, 2, 1, 1,\n",
       "       1, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2, 2, 1, 0, 0, 0, 2, 1, 0, 0, 2,\n",
       "       1, 2, 0, 0, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2]),\n",
       "     max_depth=10, n_estimators=10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
